:PROPERTIES:
:ID:       68520325-23DF-487A-A16E-F3EBA5D40B2F
:END:
#+TITLE: k8s pod eviction

* 驱逐类型
  K8S 中，驱逐类型有：
  + 节点故障处于 NotReady 状态时有 KCM 发起的驱逐
  + 节点资源压力过大时有 Kubelet 发起的驱逐
  + 抢占调度时由 Scheduler 发起的驱逐

* Taint based Eviction
  节点故障处于 NotReady 发起的驱逐可以看着是[[https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions][基于污点的驱逐]]，当节点故障长时间未上报心跳后，KCM 会标记节点为 NotReady，并根据节点上 Pod 配置的污点容忍情况开始进行驱逐。

  节点被标记为 NotReady 的时间受 kubelet 和 kcm 参数影响，主要有：
  |------------------------------+---------+--------------------------|
  | 参数                         | 组件    | 作用                     |
  |------------------------------+---------+--------------------------|
  | node-status-update-frequency | kubelet | kubelet 上报心跳的间隔   |
  | node-monitor-grace-period    | kcm     | 多久没有心跳认为节点故障 |
  |------------------------------+---------+--------------------------|

  节点被 kcm 认为故障后，会打上 =node.kubernetes.io/not-ready= 和 =node.kubernetes.io/unreachable= 的污点，并基于 Pod 污点容忍配置进行驱逐。

  注：Kubernetes 会自动给 Pod 添加针对 node.kubernetes.io/not-ready 和 node.kubernetes.io/unreachable 的容忍度，且配置 tolerationSeconds=300。

  污点类型常用的有 NoSchedule 和 NoExecute 两种，NoSchedule 是禁止调度而 NoExecute 才会触发驱逐。
  
  参考：[[https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/][污点和容忍度 | Kubernetes]]

* Zone
  K8S 会根据每个 node 对象的 region 和 zone 的 label 值，将 node 划分到不同的 zone 中，不同状态下的驱逐策略不一样，共有以下几种状态：
  1. Initial：初始状态
  2. FullDisruption：ready 的 node 数量为 0，not ready 的 node 数量大于 0
  3. PartialDisruption：not ready 的 node 数量大于 2 且其占比大于等于 unhealthyZoneThreshold
  4. Normal：上述三种状态以外的情形，都属于该状态
  
  需要注意二级驱逐速率对驱逐的影响，即 kcm 启动参数--secondary-node-eviction-rate，代表如果某个 zone 下的 unhealthy 节点的百分比超过 --unhealthy-zone-threshold（默认为 0.55）时，驱逐速率将会减小，
  如果不是 LargeCluster（zone 节点数量小于等于--large-cluster-size-threshold，默认为 50），驱逐操作将会停止，如果是 LargeCluster，驱逐速率将降为每秒--secondary-node-eviction-rate 个，默认为 0.01。

  特别需要注意的一点就是：如果集群规模小于 50，NotReady 节点大于 2，Zone 状态为 PartialDisruption 时，驱逐操作可能停止。表现上节点会打上 NoSchedule 的污点但不会有 NoExecute 的污点。

  #+begin_src go
    // pkg/controller/nodelifecycle/node_lifecycle_controller.go
    func (nc *Controller) ComputeZoneState(nodeReadyConditions []*v1.NodeCondition) (int, ZoneState) {
      readyNodes := 0
      notReadyNodes := 0
      for i := range nodeReadyConditions {
        if nodeReadyConditions[i] != nil && nodeReadyConditions[i].Status == v1.ConditionTrue {
          readyNodes++
        } else {
          notReadyNodes++
        }
      }
      switch {
      case readyNodes == 0 && notReadyNodes > 0:
        return notReadyNodes, stateFullDisruption
      case notReadyNodes > 2 && float32(notReadyNodes)/float32(notReadyNodes+readyNodes) >= nc.unhealthyZoneThreshold:
        return notReadyNodes, statePartialDisruption
      default:
        return notReadyNodes, stateNormal
      }
    }
  #+end_src

* NodePressure Eviction
  kubelet 根据驱逐信号、驱逐条件和监控间隔来进行节点压力驱逐，其中，驱逐信号是特定资源在特定时间点的当前状态：
  |--------------------+----------------------------------|
  | 驱逐信号           | 含义                             |
  |--------------------+----------------------------------|
  | memory.available   | 内存剩余可用量                   |
  | nodefs.available   | 节点磁盘剩余可用                 |
  | nodefs.inodesFree  | 节点磁盘 inodes 剩余可用         |
  | imagefs.available  | 节点镜像文件系统所在磁盘剩余可用 |
  | imagefs.inodesFree | 节点镜像文件系统 inodes 剩余可用 |
  | pid.available      | 节点 pid 剩余可用                |
  |--------------------+----------------------------------|

  驱逐条件是节点上可用资源的最小量，在不满足这个最小量时会触发驱逐，驱逐条件分为软驱逐条件和硬驱逐条件：
  + 软驱逐条件可以通过参数 =eviction-soft-grace-period= 和 =eviction-max-pod-grace-period= 定义驱逐的宽限期，在触发驱逐条件后，持续宽限期这么长的时间后才进行驱逐
  + 硬驱逐条件触发后 kubelet 会立即驱逐 pod

  监控间隔告诉 kubelet 间隔多久评估一次驱逐条件，可以通过参数 housekeeping-interval 配置，默认为 10s。

  其他参数：
  + =eviction-pressure-transition-period= 控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间，避免节点在软驱逐条件上反复横跳，默认值为 5m。

  例子：
  #+begin_src yaml
    # 硬驱逐条件
    evictionHard:
      imagefs.available: 10%
      imagefs.inodesFree: '10'
      memory.available: 100Mi
      nodefs.available: 5%
      nodefs.inodesFree: '5'
    # 避免节点在软驱逐条件上反复横跳
    evictionPressureTransitionPeriod: 3s
    # 软驱逐条件
    evictionSoft:
      imagefs.available: 15%
      imagefs.inodesFree: '100'
      memory.available: 500Mi
      nodefs.available: 10%
      nodefs.inodesFree: '50'
    # 软驱逐宽限期
    evictionSoftGracePeriod:
      imagefs.available: 2m
      imagefs.inodesFree: 2m
      memory.available: 30s
      nodefs.available: 1m
      nodefs.inodesFree: 1m
  #+end_src

  驱逐优先级：
  + 首先考虑资源使用量超过其请求的 BestEffort 或 Burstable Pod。这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出
  + 资源使用量少于请求量的 Guaranteed Pod 和 Burstable Pod 根据其优先级被最后驱逐

  参考：[[https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/][节点压力驱逐 | Kubernetes]]

* DiskPressure Eviction
  kubelet 会在 node 上内存、磁盘和可用进程标识符的负载很高或即将耗尽时对 node 上的 pod 进行驱逐，而在 DiskPressure 驱逐时常常可以看到这样的日志：
  #+begin_example
    The node was low on resource: ephemeral-storage. Container xxx was using xxxKi, which exceeds its request of 0.
  #+end_example

  这是因为 Pod 使用了 [[https://kubernetes.io/zh-cn/docs/concepts/storage/ephemeral-volumes/][临时卷（Ephemeral Volume）]]，比如 emptyDir、configMap、secret 等都属于临时卷，而大部分时候我们都不会指定 Pod 的 request.ephemeral-storage，
  因此日志会显示 =exceeds its request of 0=.

  在 Pod 的 Resource 中指定 ephemeral-storage 可以帮助 scheduler 将 pod 调度到更能满足 pod 临时存储需求的 node 上，但缺陷是如果 pod 使用的临时存储大小超过 limit，就会被驱逐。

  参考：
  + [[https://developer.aliyun.com/article/594066][体验 ephemeral-storage 特性来对 Kubernetes 中的应用做存储的限制和隔离-阿里云开发者社区]]
  + [[https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/][节点压力驱逐 | Kubernetes]]
  + [[https://kubernetes.io/zh-cn/docs/concepts/storage/ephemeral-volumes/][临时卷 | Kubernetes]]

* Clean Evicted Pods
  #+begin_src sh
    kubectl get po -A|grep Evicted|awk '{system("kubectl delete po -n" $1 " " $2)}'
  #+end_src
